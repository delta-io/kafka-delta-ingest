= kafka-delta-ingest

The kafka-delta-ingest project aims to build a highly efficient daemon for
streaming data through link:https://kafka.apache.org[Apache Kafka] into
link:https://delta.io[Delta Lake].

This project is currently highly experimental and evolving in tandem with the
link:https://github.com/delta-io/delta-rs[delta-rs] bindings.

== Features

* Multiple worker processes per stream
* Basic transformations within message
* Statsd metric output

See the link:https://github.com/delta-io/kafka-delta-ingest/blob/main/doc/DESIGN.md[design doc] for more details.

=== Example

The repository includes an example for trying out the application locally with some fake web request data.

The included docker-compose.yml contains link:https://github.com/wurstmeister/kafka-docker/issues[kafka] and link:https://github.com/localstack/localstack[localstack] services you can run `kafka-delta-ingest` against locally.

==== Starting Worker Processes

1. Launch test services - `docker-compose up setup`
2. Compile: `cargo build`
3. Run kafka-delta-ingest against the web_requests example topic and table (customize arguments as desired):

```bash
export AWS_ENDPOINT_URL=http://0.0.0.0:4566
export AWS_ACCESS_KEY_ID=test
export AWS_SECRET_ACCESS_KEY=test

RUST_LOG=debug cargo run ingest web_requests ./tests/data/web_requests \
  --allowed_latency 60 \
  --app_id web_requests \
  --transform 'date: substr(meta.producer.timestamp, `0`, `10`)' \
              'meta.kafka.offset: kafka.offset' \
              'meta.kafka.partition: kafka.partition' \
              'meta.kafka.topic: kafka.topic' \
  --auto_offset_reset earliest
```

Notes:

* The AWS_* environment variables are for S3 and are required by the delta-rs library.
** Above, AWS_ENDPOINT_URL points to localstack.
* The Kafka broker is assumed to be at localhost:9092, use -k to override.
* To clean data from previous local runs, execute `./bin/clean-example-data.sh`. You'll need to do this if you destroy your Kafka container between runs since your delta log directory will be out of sync with Kafka offsets.

==== Kafka SSL

In case you have Kafka topics secured by SSL client certificates, you can specify these secrets as environment variables.

For the cert chain include the PEM content as an environment variable named `KAFKA_DELTA_INGEST_CERT`.
For the cert private key include the PEM content as an environment variable named `KAFKA_DELTA_INGEST_KEY`.

These will be set as the `ssl.certificate.pem` and `ssl.key.pem` Kafka settings respectively.

Make sure to provide the additional option:

```
-K security.protocol=SSL
```

when invoking the cli command as well.

==== Example Data

A tarball containing 100K line-delimited JSON messages is included in `tests/json/web_requests-100K.json.tar.gz`. Running `./bin/extract-example-json.sh` will unpack this to the expected location.

===== Pretty-printed example from the file

```json
{
  "meta": {
    "producer": {
      "timestamp": "2021-03-24T15:06:17.321710+00:00"
    }
  },
  "method": "DELETE",
  "session_id": "7c28bcf9-be26-4d0b-931a-3374ab4bb458",
  "status": 204,
  "url": "http://www.youku.com",
  "uuid": "831c6afa-375c-4988-b248-096f9ed101f8"
}
```

After extracting the example data, you'll need to play this onto the web_requests topic of the local Kafka container.

NOTE: URLs sampled for the test data are sourced from Wikipedia's list of most popular websites - https://en.wikipedia.org/wiki/List_of_most_popular_websites.

===== Inspect example output

* List data files - `ls tests/data/web_requests/date=2021-03-24`
* List delta log files - `ls tests/data/web_requests/_delta_log`
* Show some parquet data (using link:https://pypi.org/project/parquet-tools/[parquet-tools])
** `parquet-tools show tests/data/web_requests/date=2021-03-24/<some file written by your example>`

=== Using Azure Event Hubs

Azure Event Hubs (with pricing tier "Standard" or higher) has a Kafka Surface that can be used with kafka-delta-ingest.

Azure Event Hubs doesn't have a local emulator, so an actual Azure Event Hubs resource is required. As a result, there's no need for the docker-compose application described above.

More info:

* https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create
* https://docs.microsoft.com/en-us/azure/event-hubs/apache-kafka-migration-guide
* https://docs.microsoft.com/en-us/azure/event-hubs/apache-kafka-troubleshooting-guide
* https://docs.microsoft.com/en-us/azure/event-hubs/apache-kafka-configurations#librdkafka-configuration-properties
* https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md#librdkafka-configuration-properties
* https://github.com/edenhill/librdkafka/wiki/Using-SASL-with-librdkafka
* https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
* https://github.com/edenhill/librdkafka/issues/3109

Notes:

* You must create an Event Hub, which is similar to a Kafka topic, in your Event Hubs Namespace. The Event Hub name is the Kafka topic name.
* Using WSL2 on Windows is recommended (OpenSSL should work out of the box).
** Using Ubuntu 20.04 is recommended, otherwise renaming temporary Delta transaction files during transaction commit will fail when using the local file system.
*** See: https://github.com/delta-io/delta-rs/blob/14a783e7e9af09914f667c7e1717b5e55aa73ede/rust/build.rs#L3-L15
*** Ubuntu 18.04 uses GLIBC 2.27 (check with `ldd --version`)
*** Ubuntu 20.04 uses GLIBC 2.31 (check with `ldd --version`)

==== Starting Worker Processes

1. Compile: `cargo build`
2. Run kafka-delta-ingest against the web_requests example topic and table (customize arguments as desired):

```bash
export AZURE_STORAGE_ACCOUNT_NAME={MY_ACCOUNT_NAME}
export AZURE_STORAGE_ACCOUNT_KEY={MY_KEY}
export AZURE_STORAGE_FILESYSTEM={MY_FILESYSTEM}
export AZURE_STORAGE_DELTA_TABLE_PATH={MY_DELTA_TABLE_PATH}
export ADLS_GEN2_TABLE_LOCATION=adls2://$AZURE_STORAGE_ACCOUNT_NAME/$AZURE_STORAGE_FILESYSTEM/$AZURE_STORAGE_DELTA_TABLE_PATH
export LOCAL_FS_TABLE_LOCATION=./tests/data/web_requests

RUST_LOG=debug cargo run ingest web_requests $LOCAL_FS_TABLE_LOCATION \
  --allowed_latency 5 \
  --kafka {MY_NAMESPACE_NAME}.servicebus.windows.net:9093 \
  --Kafka 'security.protocol=SASL_SSL' \
  --Kafka 'sasl.mechanism=PLAIN' \
  --Kafka 'sasl.username=$ConnectionString' \
  --Kafka 'sasl.password=Endpoint=sb://{MY_NAMESPACE_NAME}.servicebus.windows.net/;SharedAccessKeyName={MY_KEY_NAME};SharedAccessKey={MY_KEY}' \
  --Kafka 'socket.keepalive.enable=true' \
  --Kafka 'metadata.max.age.ms=180000' \
  --Kafka 'heartbeat.interval.ms=3000' \
  --Kafka 'session.timeout.ms=30000' \
  --Kafka 'debug=broker,security,protocol'
  --app_id web_requests \
  --transform 'date: substr(meta.producer.timestamp, `0`, `10`)' \
              'meta.kafka.offset: kafka.offset' \
              'meta.kafka.partition: kafka.partition' \
              'meta.kafka.topic: kafka.topic' \
  --auto_offset_reset earliest
```

Notes:

* The following environment variables are required by the delta-rs library:
** The AZURE_STORAGE_ACCOUNT_NAME environment variable should be just the account name, not the FQDN.
** The AZURE_STORAGE_ACCOUNT_KEY environment variable should be just the key, not the connection string.
* In the cargo command:
** Replace MY_NAMESPACE_NAME with just the namespace name, not the FQDN, of your Azure Event Hubs Namespace.
** Replace MY_KEY_NAME and MY_KEY with suitable values from your Azure Event Hubs Namespace.
** The `sasl.username` is the literal string `$ConnectionString` and not a placeholder.
** The following `--Kafka` arguments are taken from link:https://docs.microsoft.com/en-us/azure/event-hubs/apache-kafka-configurations#librdkafka-configuration-properties[here]:
*** `-X socket.keepalive.enable=true -X metadata.max.age.ms=180000 -X heartbeat.interval.ms=3000 -X session.timeout.ms=30000`
** Remove `--Kafka 'debug=broker,security,protocol'` for a cleaner console output.
** Replace $LOCAL_FS_TABLE_LOCATION with $ADLS_GEN2_TABLE_LOCATION to write to ADLS Gen2 (must prepare the file system to have the _delta_log directory and the first Delta transaction containing the schema).
* To clean data from previous local runs, execute `./bin/clean-example-data.sh`. You'll need to do this if you destroy your Kafka container between runs since your delta log directory will be out of sync with Kafka offsets.

==== Sending data to Event Hubs

On Windows, link:https://github.com/paolosalvatori/ServiceBusExplorer[Service Bus Explorer] can be used to send data to Event Hubs.

The following payload should be sent for the web_requests Delta table:

```json
{
  "status": 200,
  "session_id": "7c28bcf9-be26-4d0b-931a-3374ab4bb458",
  "method": "GET",
  "meta": {
    "producer": {
      "timestamp": "2021-03-24T15:06:17.321710+00:00"
    }
  },
  "uuid": "831c6afa-375c-4988-b248-096f9ed101f8",
  "url": "http://www.example.com"
}
```

==== Verifying data from Event Hub using kcat

kcat can be run on Windows via docker using this command, which will print the last message (-o -1).
Attention: Make sure to replace XXX with the shared access key of your Event Hub.
```
docker run -it --network=host edenhill/kcat:1.7.1 -C -o -1 -b thovoll-kdi-eh.servicebus.windows.net:9093 -t web_requests -X security.protocol=SASL_SSL -X sasl.mechanism=PLAIN -X sasl.username=$ConnectionString -X sasl.password=Endpoint=sb://thovoll-kdi-eh.servicebus.windows.net/;SharedAccessKeyName=KafkaSurfaceManageSharedAccessKey;SharedAccessKey=XXX -X socket.keepalive.enable=true -X metadata.max.age.ms=180000 -X heartbeat.interval.ms=3000 -X session.timeout.ms=30000
```
The following configuration settings in the command above are taken from link:https://docs.microsoft.com/en-us/azure/event-hubs/apache-kafka-configurations#librdkafka-configuration-properties[here]:
`-X socket.keepalive.enable=true -X metadata.max.age.ms=180000 -X heartbeat.interval.ms=3000 -X session.timeout.ms=30000`

== Kafka SSL

In case you have Kafka topics secured by SSL client certificates, you can specify these secrets as environment variables.

For the cert chain include the PEM content as an environment variable named `KAFKA_DELTA_INGEST_CERT`.
For the cert private key include the PEM content as an environment variable named `KAFKA_DELTA_INGEST_KEY`.

These will be set as the `ssl.certificate.pem` and `ssl.key.pem` Kafka settings respectively.

Make sure to provide the additional option:

```
-K security.protocol=SSL
```

when invoking the cli command as well.

== Writing to S3

When writing to S3, you may experience an error like `source: StorageError { source: S3Generic("dynamodb locking is not enabled") }`.

A locking mechanism is need to prevent unsafe concurrent writes to a delta lake directory, and DynamoDB is an option for this. To use DynamoDB, set the `AWS_S3_LOCKING_PROVIDER` variable to `dynamodb` and create a table named `delta_rs_lock_table` in Dynamo. An example DynamoDB table creation snippet using the aws CLI follows, and should be customized for your environment's needs (e.g. read/write capacity modes):


```bash
aws dynamodb create-table --table-name delta_rs_lock_table \
    --attribute-definitions \
        AttributeName=key,AttributeType=S \
    --key-schema \
        AttributeName=key,KeyType=HASH \
    --provisioned-throughput \
        ReadCapacityUnits=10,WriteCapacityUnits=10
```

For more information, see link:https://github.com/delta-io/delta-rs/tree/dbc2994c5fddfd39fc31a8f9202df74788f59a01/dynamodb_lock[DynamoDB lock].

== Developing

Make sure the docker-compose setup has been ran, and execute `cargo test` to run unit and integration tests.

== Get Involved

Join link:https://dbricks.co/delta-users-slack[#kafka-delta-ingest in the Delta Lake Slack workspace]
